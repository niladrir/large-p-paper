\documentclass[12]{article}
\usepackage{times}
\usepackage{natbib}
\usepackage{multicol}
\usepackage{hyperref}

\usepackage{amsmath, amssymb, fullpage, amsthm, array, algorithm2e,graphicx,mathtools, xparse}

\usepackage{color}
\usepackage{subfigure}
%\usepackage[dvips]{graphics}
\newtheorem{thm}{Theorem}[section]
\newtheorem{dfn}{Definition}[section]
\newtheorem{cor}{Corollary}[thm]
\newtheorem{con}{Conjecture}[thm]
%\setlength{\parindent}{0in}   % for no indent

\newcommand{\blue}{\color{blue}}
\newcommand{\red}{\color{red}}

\topmargin -0.10in   % when making pdf
\textheight 8.5in  % when making pdf

\begin{document}
%%

%%% just added to test commit

%% ================ chapter 2 starts here  ==========================

\title{Visual Statistical Inference for High Dimension, Small Sample Size Data}\label{ch:largepsmalln}
\vspace{-0.8cm}
\author{Niladri Roy Chowdhury, Dianne Cook, Heike Hofmann, Mahbubul Majumder}
%\large{This paper was presented in Joint Statistical Meetings 2011 and is submitted in JSM Proceedings 2011.} 

%\vspace{1cm}

%\normalsize

\maketitle

\begin{abstract}
Statistical graphics plays an important role in exploratory data analysis, model checking and diagnosis. Recently there were some formal visual methods for determining statistical significance of findings. We often seek to low-dimensional projections in high dimensional data which reveal important aspects of the data. Projection pursuit for classification finds projections that reveal differences between classes. In this paper we are interested in the performance of classification methods when the number of observations is relatively small compared to the number of variables, known as a large $p$ (dimension) small $n$ (sample size) problem using visual statistical inference. We apply projection pursuit for classification to pure noise data and to the data when there is some separation. We use the lineup protocol \cite{buja:2009} to make comparisons among the pure noise data and the data which has some separation.

\end{abstract}

{\color{red} The main message of this paper are :
\begin{itemize}
\item \texttt{tourr} package in R is new. When we used the guided tour function in the \texttt{tourr} package to do projection pursuit for getting low dimensional projections of a high dimensional noise data (data with no real separation), surprisingly we could often pick the real data from a lineup. So we want to check the optimization procedure of the \texttt{tourr} package.
\item  We notice that as we increase the number of dimensions($p$) for a fixed sample size($n$), the groups separate out even for a noise data. So we believe that when $p ~ n$, the separation among the groups obtained by doing a LDA may not be real. So we want to test whether the separation among the groups is real or fake.
\item Finally  since the groups start separating out as we increase $p$ for a fixed $n$ for noise data, we would like to obtain a probable range for the number of dimension at which the groups start separating. 
\bigskip

\centerline{\bf *** From Di ***}

\item Visual inference can be used to educate the broader community about HDLSS issues. The example from the Toth paper shows this, that what is seen as separation is consistent with noise when we do visual inference. Visual inference can help researchers understand the issues of HDLSS more easily. (Primary message.)
\item Visual inference can be used to examine algorithms for dimension reduction in HDLSS. Projection pursuit algorithm needs substantive optimization but checking that it is working correctly is difficult. It requires visual examination of the result - so compare real vs no separation using visual inference - can help to determine if the optimization algorithm is doing its job.
\end{itemize} }

\section{Introduction} 
%testing

%topic
%{\color{red} History of visualization. Why statistical graphics is important.}
{\blue TOPIC: What's important about this paper? HDLSS}

Many problems needing solutions today require the analysis of data where more variables are measured than samples are taken. This is commonly referred to as high dimensional, low sample size (HDLSS) data (\cite{hall:2005} and \cite{marron:2007}). %\ref{XXX}  
HDLSS data occur in many application areas like face recognition and gene expression data. Classical statistical methods often fail in this context, because there is insufficient data to be able to estimate properties of matrices, such as the variance-covariance matrix, required by many methods. 

{\blue TOPIC: Explanation of problem with classical methods}

Reducing the dimension would seem to be the natural first step in HDLSS data. Principal component analysis (PCA) is the classical approach. PCA requires estimating the eigenvalues (maximum variance) and eigenvectors (direction of maximum variance) of the population variance-covariance based on the sample. With insufficient data this is a Sisyphean task. Just imagine, estimating a line on the foundation of a single point. There are infinitely many lines possible to return. Similarly for classification tasks, finding a low-dimensional representation of the separation between classes is a common first step. Linear discriminant analysis (LDA) is the classical method for this. LDA finds the low-dimensional space where the classes are most separated, by solving an eigen decomposition problem comparing distances between class means with variance around each mean. When there are few sample points differences between classes can be found in many different low-dimensional spaces. 

{\blue*** No need to use ``\verb#\\#'' after every paragraph}


{\blue TOPIC: Now discuss the contemporary literature on HDLSS adjustments to methods. This paragraph needs more work}

\cite{marron:2007} describes the estimation issues associated with HDLSS.  
One of the problems with HDLSS dataset is that not all the measured variables are ``important'' for understanding the underlying phenomenon of interest. It is important in many applications to reduce the number of dimension of the original data prior to any modeling of the data. There are many established methods of dimension reduction like principal component analysis (PCA), factor analysis, projection pursuit, principal curves, self-organizing maps and many others. Many of the above use linear dimension reduction techniques for normal variables. Others use higher-order dimension reduction methods for datasets which are not realizations of Gaussian distributions.  Many advancements in the PCA to handle HDLSS data has been done by \cite{marron:2011} and \cite{yata:2010}. On the other hand, \cite{donoho:2009} and \cite{donoho:2008} studies the optimal variable selection and introduces a principle of model selection based on the notion of higher criticism in situations where only a small fraction of the variables are useful and unknown and contributes weakly to the classification decision.\\
%topic

{\blue TOPIC: This is clearly not getting the problem understood.}

Clearly, though, the issues of working with HDLSS data are not clear to many researchers. In \cite{toth:2010} LDA is used to examine gene expression data of wasps. Figure \ref{oligo} shows the result. There are 50 different paper wasps divided into 4 types: Foundress (F), Gyne (G), Queen (Q) and Worker (W). There are 14  wasps of type Foundress and 12 each of the other 3 types. The authors, knowing that LDA requires that the number of observations ($n$) should be larger than ($p$), first reduced the dimension from 447 to 40 by randomly selected a subset of significantly different oligonucleotides. This is the almost same approach used in \cite{dudoit:2002}, by the way, in one of the first studies of classification of gene expression data. What results is a picture of the four groups that suggests big differences in the types of wasps. With visual inference it can be shown that there is no real difference between the classes - what you see is a mirage. Visual inference methods will explain why. 

\begin{figure*}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{.45}{\includegraphics{toth_lda.pdf}}
       \caption{LD1 versus LD2 from an LDA on a randomly selected subset of 40 significantly different oligos. F, Foundress; G, gyne; Q, queen and W, worker.}
     \label{oligo}
%\end{figurehere}
\end{figure*}  

%Any statistical analysis must include some statistical graphics. For exploratory data analysis, statistical graphics play an invaluable role in model checking and diagnostics. Even though we have established mathematical procedures to obtain various statistics, we need to support the results by also producing the relevant plots. \\
%topic
%{\color{red} Recent advancements in statistical graphics. How statistical graphics has been recently used as a tool for statistical inference.}\\
%In recent times there have been major advances in statistical graphics. Modern computing systems like R and SAS produce high quality statistical graphics. 


%{\color{red} Explanation of Large p, Small n}\\
%A challenging case of data mining is the high dimensional, low sample size (HDLSS) data. In HDLSS  settings, the number of dimensions of the data is larger or often much larger than the number of observations. HDLSS data occur in many applied areas like microarray analysis, face recognition and gene expression data. As pointed out in \cite{marron:2007}, classical multivariate statistical methods often fail to give a meaningful analysis in HDLSS contexts. 


%Let us consider the following example of LDA method used to classify the groups of paper wasps. There are 50 different paper wasps divided into 4 groups: Foundress (F), Gyne (G), Queen (Q) and Worker (W). There are 14 paper wasps of type Foundress and 12 each of the other 3 types. The authors, knowing that LDA requires that the number of observations ($n$) should be larger than ($p$), randomly selected a subset of 40 significantly different oligonucleotides from a total of 447 oligonucleotides. Figure \ref{oligo} shows the scatterplot of LD1 versus LD2.

%\newpage

%Figure \ref{oligo} clearly shows that groups Foundress(F) and Gyne(G) are two separated clusters while the groups Worker(W) and Queen(Q) are not separated and forms a separate cluster. It would be interesting to find whether the separation is real or fake.

%topic
{\color{red} Outline of the paper.}

This paper describes visual inference as applied to dimension reduction for supervised classification problems. In particular we focus on dimension reduction using projection pursuit, and the affect that having large dimension has on the robustness of separation between classes.  Small simulation experiments are used to examine the problem in a controlled setting. Visual inference also is used to check the operation of the projection pursuit optimization. The next section explains the methods behind visual inference. Section \ref{sec:theory} describes theoretically what will happen with dimension reduction of HDLSS data containing two groups. Section \ref{sec:experiment} discusses the experiment designed to examine people's perception of separation in the presence of real separation and purely noise for simulated HDLSS data, and the performance of the optimization algorithm. The wasps data is revisited at the end of the paper. 
%In this paper we check the projection pursuit optimization on pure noise with no real separation by using statistical graphics in Section \ref{sec:largep}. We also check the PP optimization on dimensions with some real separation and present results. In Section  \ref{sec:distance} we are also concerned about how the distance between the means increase with $p$ for a fixed n. We also present a probable range for the number of dimensions at which the clusters starts to separate out for both one dimensional and two dimensional projections.


%\section{Visual Statistical Inference} \label{sec:visual_test} 
%
%This section outlines the concepts of visual inference in comparison to the procedures of classical statistical inference. 
%
%Let $\theta$ be a population parameter of interest, with $\theta \in \Theta$, the parameter space. 
%Any null hypothesis $H_0$ then partitions the parameter space into $\Theta_0$ and $\Theta_0^c$, with $H_0: \theta \in \Theta_0$ versus $H_1: \theta \in \Theta_0^c$. 
%% In hypothesis testing terminology, the parameter space $\Theta$ of a population parameter $\theta$, can be partitioned into $\Theta_0$ and $\Theta_0^c$. We test $H_0: \theta \in \Theta_0$ versus $H_1: \theta \in \Theta_0^c$.
%
%%\subsection{Visual Statistic} 
%
%Unlike classical hypothesis testing, the statistic in visual inference is not a single value, but a plot that is appropriately chosen to describe the parameter of interest, $\theta$. When the alternative hypothesis is true, it is expected that the plot of the observed data, the test statistic, will have visible feature(s) consistent with $\theta \in \Theta_0^c$, and that visual artifacts will not distinguish the test statistic as different when $H_1$ is not true.
%
%\begin{dfn}\label{dfn:lplot}
%A lineup plot is a layout of $m$ visual statistics, consisting of 
%\begin{itemize}\itemsep-3pt
%\item $m-1$ plots simulated from the model specified by $H_0$  (null plots) and 
%\item the test statistic produced by plotting the observed data, possibly arising from $H_1$.
%\end{itemize}
%\end{dfn}
%
%If $H_1$ is true, the test statistic is expected to be the plot that is most different from the other plots in the lineup plot. A careful visual inspection should reveal the differences in the feature shown by the test statistic under null and alternative hypothesis. {\em If the test statistic cannot be identified} in the lineup, the conclusion is to {\em not reject the null hypothesis.} The $(m-1)$ null plots can be considered to be samples drawn from the sampling distribution of the test statistic assuming that the null hypothesis is true.
%
%
%Since the lineup plot consists of $m$ plots, the probability of choosing any one of them is $1/m$. Thus we have type-I error probability of $1/m$.
%
%The lineup plot can be evaluated by one or more individuals. When a single individual identifies the observed graph in the lineup plot we report a $p$-value smaller than $1/m$, otherwise the $p$-value is larger than $1/m$. 
%
%If $N$ individuals evaluate a lineup plot independently, we count the number of successful evaluations as $U \sim \text{Binom} (N,\frac{1}{m})$ and report a  $p$-value of at most $Pr(U \ge u)= \sum_{k \ge u}^N {{N \choose k} (\frac{1}{m})^k(1-\frac 1m)^{(N-k)}}$ where $u$ is the observed number of successful evaluations. %Notice that when $N=1$, this $p$-value is $\frac1m$. 
%
%
%For two different visual test statistics of the same observed data, the one  is better, in which a specific pattern is more easily distinguishable visually.

%\section{Inference for the means of two populations} \label{sec:largep}
\section{Visual inference methods} \label{sec:inference}

\cite{buja:2009}, following from \cite{gelman:2004}, proposed two protocols that allow the testing of discoveries made from statistical graphics. Among the two protocols explained in \cite{buja:2009} we are particularly interested in the lineup protocol. The lineup protocol is particularly used for testing significance of findings. Like any statistical test visual test is also associated with a test statistic. But unlike classical hypothesis testing, the test statistic in visual inference is not a real number, but a plot that is appropriately chosen to describe the parameter of interest. The plot of the observed data which is our test statistic is placed randomly among a set of ($m$ - 1) null plots. These type of a plot is known a lineup plot of $m$ plots. The null plots are generated by a method consistent with the null hypothesis. Human subjects are asked to identify the plot which has the most distinct feature(s). If the human subjects can identify the plot of the observed data, we reject the null hypothesis. When the alternative hypothesis is true, it is expected that the plot of the observed data, the test statistic, will have visible feature(s) inconsistent with the null hypothesis and human subjects will be able to identify the plot of the observed data as different from all the other null plots. For more details see \cite{buja:2009}. 

Visual Inference can be used to test a number of hypothesis for which a classical test is unknown till date. Later in 2012, \cite{majumder:2011} does a comparative study of the empirical power of the classical test and the visual test and shows that visual test does a reasonable job in a classical situation. But in situations where the classical test is unknown till date, the visual test still maintains a good power. 


%, and that visual artifacts will not distinguish the test statistic as different when the alternative hypothesis is not true.

{\blue *** Fill this section in. Doesn't need too much, needs to have basics and refer to Buja and  Mahbub's paper, and your tech reports.}

%\section{Theoretical expectation of dimension reduction for HDLSS data} \label{sec:theory}

%\subsection{Motivation}

\section{Explanation of the methods}

 
\subsection{Dimension reduction methods}

{\color{red} Explanation of Projection Pursuit(Dimension Reduction)} \\
In this paper we used the idea of projection pursuit which was implemented by \cite{friedman:1974}. Projection pursuit(PP) is a statistical tool to find the most interesting low dimensional projections from high dimensional to low dimensional space that reveals the most details about the structure of the data. In this paper we mainly use the one and two dimensional projections obtained by projection pursuit on higher dimensions.  As pointed out in \cite{huber:1985} the most exciting feature of projection pursuit is that it can bypass the curse of dimensionality. Hence it works really well in HDLSS situations. The other methods fail to pick up small features unless the sample size is large. Also projection pursuit are able to ignore noisy and non-informative variables where it is in advantage over minimal spanning trees, multidimensional scaling and most clustering methods. Though these methods work in a HDLSS situation, they cannot ignore the ``noise" variables. The  major drawback of PP method is its demand of the computer time. But in times of super computer and high speed computing this does not seem that big an issue.

PP method needs a projection index on the basis of which it finds the low dimensional projections of a high dimensional data. Various indices are used for this like Linear Discriminant Analysis (LDA) index, Quadratic Discriminant Analysis (QDA) index and many others. But in this paper we use Penalized Discriminant Analysis (PDA) index described in \cite{lee:2009}. PDA index is an improvement over the LDA index in a HDLSS situation. 

%topic
{\color{red} Explanation of \texttt{tourr} package(Software Used)} \\
In this paper we use the \texttt{tourr} package in \texttt{R} \cite{r} by \cite{WC08}. The \texttt{tourr} package produces tours of multivariate data. The package also includes functions for creating different types of tours like grand, guided and little tours, which project multivariate data with $p$ dimensions to 1, 2, 3 or $d$ dimensions where $d \le p$. In this paper we mainly use the guided tour function which instead of picking a new projection completely at random, it picks projections that are closer to the current projection, so that we eventually converge to a single maximally interesting projection. The \texttt{tourr} package comes with different indices like cmass, holes, and LDA index but we use the PDA (Penalized Discriminant Analysis)  index described in \cite{lee:2009} which is specifically designed to handle HDLSS problems. \\     

%{\color{red} Statement of the problem. What we want to show. }\\

%The optimization procedure in the  \texttt{tourr} package is new, different from the algorithms that have existed before in GGobi \cite{STLBC03} and XGobi \cite{SCB91}. 
%We have been using PP with the PDA index, on a large $p$, small $n$ data set from a microarray study, and for comparison have been looking at pure noise data. As strange as it might seem it seemed like we could often pick the projection of the  ``original'' data from a lineup of projections of permuted class data. This should not be possible.
%
%The distance between clusters obtained by doing a LDA or a PDA on a noise data increases as we increase the number of dimensions $p$ for a fixed sample size $n$. So another motivation for this paper is in situations where the number of dimensions ($p$) is approximately equal or close to the number of observations ($n$), we may see separation among the groups when a linear discriminant analysis (LDA) or a penalized discriminant analysis (PDA) is performed. A natural question is whether there is actually a real separation among the groups or we observed the separation due to the fact that $p \approx n$ or $p > n$. 
%
%To check the optimization procedure we looked at the absolute difference of the means of random noise divided into two groups. We start with one dimensional data of random noise and assign two groups. Since we are interested in the projections, we work with the absolute differences of the means. \\

\subsection{Experimental Setup} \label{sec:experiment}

{\blue*** Explain the Turk experiment}

An experiment is designed to study the ability of human observers to detect the effect of one or two dimensions of real separation in $p$ dimensions of noise. Data is simulated for different values of $p$  ( = 20, 40, 60, 80, 100).  So two groups of $p$ dimensions of data with sample size $n = 15$ were generated from Normal(0 ,1). We label the data from the first group as class 1 and the data from the second group as class 2. So we have a 30 $\times$ ($p$ + 1) matrix , say, $X$,  where the first 15 observations are from class 1 and the last 15 observations are from class 2.  So we can write X as
$$X^{n \times (p + 1)} = (X_1, X_2, \dots, X_p, \hbox{Class})$$ where each $X_i$ is a vector of dimension 30 for $i = 1, \dots, p$. This matrix $X$ excluding the Class variable gives the $p$-dimensional noise data with 2 classes.

To bring in the real separation in the noise data, we subtract 3 from the elements of the $p$-th variable $X_p$ which belongs to class 1 and add 3 to the elements of the same variable which belongs to class 2. So we have
$$
X_p = \left\{ \begin{array}{rl}
 X_p - 3 &\mbox{ if $X_p \in$ class 1} \\
 X_p + 3 &\mbox{ if $X_p \in$ class 2}
       \end{array} \right.
$$
We do this so that the mean of the observations in the two classes in the $p$-th dimension is separated by 6 units. This modified $X$ matrix works as the data with one dimension of real separation. 

So we obtain two sets of data with $p$ dimensions with 30 observations in each dimension divided into two classes. One of the datasets is pure noise and the other has some real separation in the form of the $p$-th dimension. On each of these datasets of $p$ dimension, we perform a projection pursuit optimization with a PDA index to obtain the $d = 1$ dimensional projections.  

We repeat the above procedure to obtain the $d = 2$ dimensional projections as well. But instead of labeling the 15 observations as class 1 and class 2, we assign the first 10 observations as class 1, the second 10 observations as class 2 and the last 10 as class 3. Once again we have a 30 $\times$ ($p$ + 1) matrix , say, $X$,  where the first 10 observations are from class 1, the second 10 observations are from class 2 and the last 10 observations are from class 3.  So we can write X as
$$X^{n \times (p + 1)} = (X_1, X_2, \dots, X_p, \hbox{Class})$$ where each $X_i$ is a vector of dimension 30 for $i = 1, \dots, p$. This matrix $X$ excluding the Class variable gives the $p$-dimensional noise data with 3 classes.

As before to bring in the real separation, we adjust the means of the 3 classes in the last two dimensions i.e. $X_{p-1}$ and $X_p$. We do the adjustment in the following way:
$$
(X_{p-1}, X_p) = \left\{ \begin{array}{rl}
 (X_{p-1} - 3, X_p) &\mbox{ if $(X_{p-1}, X_p) \in$ class 1} \\
 (X_{p-1} + 3, X_p) &\mbox{ if $(X_{p-1}, X_p) \in$ class 2} \\
 (X_{p-1} , X_p + \sqrt{27}) &\mbox{ if $(X_{p-1}, X_p) \in$ class 3}
       \end{array} \right.
$$
If we plot $X_{p-1}$ versus $X_p$ in a scatterplot, the points cluster along the vertices of an equilateral triangle of side 6. As before, we applied the projection pursuit to obtain the two dimensional projections. In this way we obtain the data with 2 dimensions of real separation divided into 3 classes. 
So for obtaining the two dimensional projections we have two sets of data - one with random noise and the other with two dimensions of real separation. Again on each of the above datasets we perform a projection pursuit with a PDA index to obtain the $d = 2$ dimensional projections. 
Finally we have 4 different sets of data for different values of $p$. The different combinations of sample size $n$, dimension $p$, projection $d$ and presence of noise were generated as shown in Table \ref{freq}. Three replicates of each level were generated. These produced 60 different ``observed data sets''. 

\begin{table}[htbp]
\begin{center}
\caption{Values of parameters considered for the experiment.}
\begin{tabular}{cccp{3cm}}
  \hline
  \hline
  $n$ & projection($d$) & presence of real separation & dimension($p$) \\
  \hline
  15 & 1 & Yes & 20, 40, 60, 80, 100 \\
      & & No & 20, 40, 60, 80, 100\\
   10 & 2 & Yes & 20, 40, 60, 80, 100 \\
     & & No & 20, 40, 60, 80, 100\\   
      \hline
\end{tabular}
\label{freq}
\end{center}
\end{table}


To obtain a lineup with $m = 20$ plots for one dimensional projections for random noise, we plot the one dimensional projections on the horizontal axis obtained from the projection pursuit with the PDA index and a fixed value (1.5 in this case) on the vertical axis colored by the class variable. We call this plot the ``observed plot" as it is obtained from the observed dataset. We adjusted the limits of the vertical axis so that the points form circular clusters instead of bands of points. The 19 null plots in the lineup were obtained by  permuting the class variable so that breaking any dependency between the class variable and the other variables. Then we placed the ``observed plot" randomly among the 19 null plots to obtain one lineup. Two different statistics Wilk's $\lambda$ (\cite{JW02})  and the within sum of squares to between sum of squares ratio (BW ratio) were calculated for each of the 20 plots in the lineup. 

The above procedure was repeated for the one dimensional projections for data with one dimension of real separation. Figure \ref{fig:test_category_1d} gives the lineup plot for the one dimensional projections with $p = 20$ dimensions where 19 dimensions are pure noise data and 1 dimension has real separation. This is an example of a lineup that we used in our experiment. 
%{\color{red} The data recorded for the participants. How many participants were recruited.} \\

% and Figure \ref{not_noise_1d} gives the lineup plot for the one dimensional projections for 99 dimensions of pure noise and 1 dimension of real separation.
% and Figure \ref{not_noise} gives the lineup plot for the two dimensional projections for 98 dimensions of pure noise and 2 dimensions of real separation.

%\end{multicols}
%
\begin{figure}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{1.00}{\includegraphics{plot_real_1d_20_11.pdf}}
       \caption{Lineup plot ($m=20$) with 19 dimensions of random noise  and 1 dimension of real separation for testing $H_0$: There is no difference among the plots. When the alternative hypothesis is true the observed plot should have the largest separated colors. Can you identify the observed plot?}
     \label{fig:test_category_1d}
%\end{figurehere}
\end{figure}
%
%
%\begin{figure}[hbtp]
%%\begin{figurehere}
%   \centering
%       \scalebox{1.00}{\includegraphics{plot_1d_99_19.pdf}}
%       \caption{Lineup plot ($m=20$) with 99 dimensions of pure noise and 1 dimension of real separation for testing $H_0$: There is no difference among the plots. When the alternative hypothesis is true the observed plot should have the largest separated colors. Can you identify the observed plot?}
%       \label{not_noise_1d}
%\end{figure}

Similar procedure was followed to obtain the lineup plot for two dimensional projections for random noise and data with two dimensions of real separation. But in this case we plot the first two projections on the horizontal and vertical axis respectively colored by the class variable to get a scatterplot. This again gives the ``observed plot". To obtain the null plots, we permute the class variable and perform the projection pursuit on the permuted dataset. The ``observed data" is placed randomly among the 19 null plots to obtain a lineup of $m = 20$ plots. Figure \ref{fig:test_category} gives the lineup plot for the two dimensional projections for $p =100$ dimensions where all the 100 dimensions are pure noise data. As before we calculated the Wilk's $\lambda$ (\cite{JW02})  and the within sum of squares to between sum of squares ratio (BW ratio) for each of the 20 plots in the lineup. 
 
\begin{figure}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{1.00}{\includegraphics{plot_2d_100_15.pdf}}
       \caption{Lineup plot ($m=20$) with 100 dimensions of random noise for testing $H_0$: There is no difference among the plots. When the alternative hypothesis is true the observed plot should have the largest separated colors. Can you identify the observed plot?}
       \label{fig:test_category}
%\end{figurehere}
\end{figure}


%\begin{figure*}[hbtp]
%%\begin{figurehere}
%   \centering
%       \scalebox{1.00}{\includegraphics{plot_2d_98_5.pdf}}
%       \caption{Lineup plot ($m=20$) with 98 dimensions of pure noise and 2 dimensions of real separation for testing $H_0$: There is no difference among the plots. When the alternative hypothesis is true the observed plot should have the largest separated colors. Can you identify the observed plot?}
%       \label{not_noise}
%\end{figure*}


{\color{red} Description of the procedure. }\\

\subsection{Theoretical explanation of the choice of dimensions} \label{sec:theory}

To decide on the different dimensions, we looked at the distribution of the absolute difference of the means of random noise divided into two classes. In this case we start with one dimension of random noise divided into two class and look at the means of the observation in each class. Since we are interested in the projections, we work with the absolute differences of the means.

Let us denote $X_{ij}$ as the $j$-th observation in the $i$-th class where $j = 1, \dots, n$. Essentially in our case we have only $i = 2$ class. $X_{ij}$s are random noise obtained from a standard normal distribution. The difference between the means of the absolute noise for the two classes $k$ and $l$ is given by $\bar{X}_{k.} - \bar{X}_{l.}$ and $$\bar{X}_{k.} - \bar{X}_{l.} \sim \hbox{Normal}(0, 2/n)$$ Let us define $$Y  = |\bar{X}_{k.} - \bar{X}_{l.}|$$ where $Y \sim \hbox{Half Normal}$ with scale parameter $ \sigma = \sqrt{2/n}$. \\

We can find the expectation and the variance of $Y$: 
$$E(Y) = \sigma \sqrt{2/\pi}$$
$$Var(Y) = \sigma^2 (1 - \frac{2}{\pi})$$

Let us consider $p$ dimensions of random noise, each dimension being divided into $i = 2$ groups with $n = 15$ observations. Let us define $$Y_m = |\bar{X}_{m1.} - \bar{X}_{m2.}|$$ where $X_{mij}$ is the $j$-th observation in the $i$-group for the $m$-th dimension. We can obtain the sum of the absolute difference between the means for all the $p$ dimensions. So we define $$Y = \sum_{m=1}^p Y_m = \sum_{m=1}^p |\bar{X}_{m1.} - \bar{X}_{m2.}| $$ \\

Assuming independence among the dimensions of pure noise,

$$E(Y) = p \sigma \sqrt{2/\pi}$$
$$Var(Y) = p \sigma^2 (1 - \frac{2}{\pi})$$

To check for the optimization procedure we bring in one dimension of real separation in the data. So we adjust the mean of $X_{mij}$s such that in the $p$-th dimension, we have $$Y_p = |\bar{X}_{p1.} - \bar{X}_{p2.}| = 6$$ So we define $Z$ as the sum of the absolute differences of the mean with one dimension of real separation as $$Z = \sum_{m=1}^{p-1} |\bar{X}_{m1.} - \bar{X}_{m2.}| + Y_p = \sum_{m=1}^{p-1} |\bar{X}_{m1.} - \bar{X}_{m2.}| + 6$$

Again assuming independence among the dimensions of pure noise and real separation,

$$E(Z) = (p - 1) \sigma \sqrt{2/\pi} + 6$$
$$Var(Z) = p \sigma^2 (1 - \frac{2}{\pi})$$

As we increase the value of $p$ for a fixed $n$, the spread of $Y$ increases with a factor of the dimension $p$ as the spread of $Z$ increases as well. The mean of both $Y$ and $Z$ also increases with a factor of $p$ but the expected value of the difference between $Y$ and $Z$ stays constant and does not depend on the number of dimensions ($p$). 
$$E (Z - Y) = (p - 1) \sigma \sqrt{2/\pi} + 6 - p \sigma \sqrt{2/\pi} = 6 - \sigma \sqrt{2/\pi}$$ \\
 Figure \ref{fig:dimen} shows the phenomenon.  \\

%\end{multicols}
%
\begin{figure}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{0.70}{\includegraphics{sum-noise-real-1.pdf}}
       \caption{Plot showing the distribution of the sum of absolute difference of means for data with pure noise ($Y$) and real separation ($Z$) for different values of the dimension. The distribution of $Y$ and $Z$ are colored in red and green respectively with the blue line showing the 5th percentile of $Z$. The blue area shows the area of $Y$ which is greater than the 5th percentile of $Z$. We can notice that the blue area increases as we increase the number of dimensions. }
     \label{fig:dimen}
%\end{figurehere}
\end{figure}

In Figure \ref{fig:dimen} we can see as we increase the number of dimensions ($p$) from 20 to 100, the region of the distribution of $Y$ which is greater than the 5th percentile of $Z$ increases as the variability in $Y$ and $Z$ increases as a factor of the dimension ($p$).

% As a result, the probability of obtaining two clusters for data with pure noise indicating that for a large dimension, the sum of the absolute differences of the means for random noise would be close to the sum of the absolute differences for the means with only one dimension of real separation. 

The next step is to find the number of dimension for a specific value of the common region between the two distributions. To allow us some error, we look the value of the number of dimension $p$ for which $$\hbox{P}[Y > Z_{\alpha}] = \delta$$ where $Z_{\alpha}$ is the $\alpha$-th percentile of $Z$.  So for a given value of $\delta$, we can find the value of $p$ for which the above condition holds true for $\alpha = 0.05$.

We did the above described procedure for 5 different values of $\delta  = 0.0000001, 0.01, 0.05, 0.1$ and $0.2$. We repeated the above procedure 100 times to get 100 values of each dimension $p$ corresponding to each $\delta$. Table \ref{tab:dimen} shows the summaries of the number of dimensions for each value of $\delta$.

\begin{table}[htbp]
\begin{center}
\caption{Numerical Summaries of dimension $p$ for each value of $\delta$.}
\begin{tabular}{cccp{3cm}}
  \hline
  \hline
  $\delta$ & Median & 5th percentile & 95th percentile \\
  \hline
  0.0000001 & 24 & 19 & 28 \\
      0.01 & 41 & 38 & 44\\
   0.02 & 61 & 56 & 64 \\
     0.1 & 77 & 72 & 81\\   
     0.2 & 106 & 99 & 112\\ 
      \hline
\end{tabular}
\label{tab:dimen}
\end{center}
\end{table}

%The optimization is checked for 1D and 2D projections. For 1D projections data of size 30 is simulated from a standard normal distribution with $ p=20, 40, 60, 80$ and $100$ which are close to the values obtained in Table \ref{tab:dimen}. The first 15 observations are labeled class 1 and the second 15 observations are labeled class 2. There is no true class, it is pure noise data -- any separation seen between classes in the projection are purely due to the high dimension. We compute projection pursuit using the PDA index and plot the one dimensional projections in a lineup plot with $m=20$ where alternatives plots are created by randomly permuting the class labels. We expect that the original data should not be detectable -- it's just noise, any labeling of the observations to class is random. 

%topic
%{\color{red} Statement of the problem. What we want to show. }\\
%Consider two random samples $X_{i1}, X_{i2}, \dots, X_{ip}$ for $i = 1$ and 2 which have means $\mu_1 = (\mu_{11}, \mu_{12}, \dots, \mu_{1p})^T$ and $\mu_2 = (\mu_{21}, \mu_{22}, \dots, \mu_{2p})^T$ respectively. We consider testing the following high-dimensional hypothesis :
%
%$$H_0 : \mu_1 = \mu_2 \qquad \hbox{versus} \qquad H_a : \mu_1 \neq \mu_2 .$$
%
%The hypothesis $H_0$ consists of the $p$ marginal hypotheses $H_{0l} : \mu_{1l} = \mu_{2l}$ for $l = 1, \dots, p$ regarding the means on each data dimension.
%
%%The optimization procedure in the  \texttt{tourr} package is new, different from the algorithms that have existed before in GGobi \cite{STLBC03} and XGobi \cite{SCB91}. 
%%We have been using PP with the PDA index, on a large $p$, small $n$ data set from a microarray study, and for comparison have been looking at pure noise data. As strange as it might seem it seemed like we could often pick the projection of the  ``original'' data from a lineup of projections of permuted class data. This should not be possible. 
%
%\subsection{Procedure}
%topic

%For testing the above hypothesis, we simulate two sets of $p$-dimensional data from a standard normal distribution with $n = 15$ observations in each dimension in each set. Consider $Z_{ij} = (Z_{i1}, Z_{i2}, \dots, Z_{ip})$ for $i = 1$ and 2 and $j = 1, \dots, p$ are simulated from a $N(0, 1)$ distribution. These  two sets of data should represent the random samples collected from the two population. To test for the equality of means, we bring in some separation in the last dimension by adding and subtracting a value $\delta$ from the mean of the dimensions.  
%
%We define $X_{1p} = - \delta + Z_{1p} $ and $X_{2p} = \delta + Z_{2p}$. 
%We bring in a class variable which indicates the population the dataset is obtained from. The first 15 observations are labeled class 1 and the second 15 observations are labeled class 2.  We compute projection pursuit using the PDA index and plot the one dimensional projections in a lineup plot with $m=20$ where alternatives plots are created by randomly permuting the class labels.  We showed the lineup plots to several subjects and asked them to identify the plot which has the largest separation among the groups.
%
%To compare the above method, we again simulate two sets of $p$-dimensional data from a standard normal distribution with $n = 15$ observations in each dimension in each set. So now we have $X_{ij} = Z_{ij}$ for $i = 1$ and 2 and $j = 1, \dots, p$. So the two datasets from the two population do not have any real separation and they are pure noise. We again compute using the PDA index and plot the one dimensional projections in a lineup plot with $m=20$ where alternatives plots are created by randomly permuting the class labels. Several individuals were again asked to identify the plot which has the largest separation among the groups.\\[1cm]
%
%
%{\color{blue}Do not know how to write the two dimensional projections in this set up. May be test for equality of mean vectors for 3 populations ??}


%For 2D projections data is simulated from a standard normal distribution with $ p=20, 40, 60, 80$ and $100$ with 30 observations in each dimension. The first 10 observations are labeled class 1, the second 10 observations are labeled class 2 and the last third are labeled class 3. There is no true class, it is pure noise data. To account for the occasional convergence problem with the optimization 30 null plots are generated. The 19 null plots which have the smallest Wilks $\lambda$ (\cite{JW02}) values are used for the lineup. 
%
%For comparison, for each of these examples we also simulated data that had real classes, too. We also simulated data with ($p$ - 1) dimensions of pure noise and one dimension of real separation, in all making it $p$ dimensions. In the same way, each dimension has  30 observations and they were divided into 2 classes. The separation was designed in a way such that if we plot the one dimension with real separation, the points lie approximately 6 units apart. Once again the first 15 observations are labeled class 1 and the last 15 observations are labeled class 2. In this case there is one dimension of real separation. We once again compute the projection pursuit using the PDA index and plot the one dimensional projections.
%
%For 2D projections, data is simulated $p$ dimensions of pure noise. Again each dimension has  30 observations with 3 classes. But in this case the last two dimensions are changed so that the data has some real separation. The two dimensions with real separation was constructed such that the points lie close to the vertex of an equilateral triangle of side 6 units. Once again the first 10 observations are labeled class 1, the second 10 observations are labeled class 2 and the last third are labeled class 3. Here we have two dimensions of real separation. We again compute projection pursuit using the PDA index and plot the two dimensional projections in a lineup plot.  To account for the occasional convergence problem of optimization we select the 19 plots with the minimum Wilk's $\lambda$ from the generated 30 plots generated.



%\begin{multicols}{2}

%\section{Visual Inference and HDLSS Experiment}\label{sec:experiment}
%%topic
%\subsection{Procedure}

%{\color{red} Description of the experiment. How the experiment was conducted. What are the parameters, how many different plots.} \\
%An experiment is designed to study the ability of human observers to detect the effect of one dimension of real separation in $p$ dimensions of noise. Data is simulated for different values of $p$  ( = 20, 40, 60, 80, 100) with sample size $ n  =  15$. For bringing in the real separation, we fixed $\delta $ = 3. So two sets of $p$ dimension of data with sample size $n = 15$ were generated from Normal(0 ,1) for the two populations.  $\delta = 3$ was added and subtracted to the $p$-th dimension to bring in the largest separation.  We also repeated the above procedure for the different $p$ dimensions of pure noise with no real separation ($\delta = 0$). Data sets with different combinations of $n$, $p$ and presence of noise were generated with frequencies shown in Table \ref{freq}. Three replicates of each level were generated.These produced 30 different ``observed data sets''. 
%
%%({\color{blue} only for one dimensional projections.}) 
%
%\begin{table}[htbp]
%\begin{center}
%\caption{Values of parameters considered for the experiment.}
%\begin{tabular}{cccp{3cm}}
%  \hline
%  \hline
%  $n$ & projection & presence of real separation & $p$ \\
%  \hline
%  15 & 1 & Y & 20, 40, 60, 80, 100 \\
%      & & N & 20, 40, 60, 80, 100\\
%   10 & 2 & Y & 20, 40, 60, 80, 100 \\
%     & & N & 20, 40, 60, 80, 100\\   
%      \hline
%\end{tabular}
%\label{freq}
%\end{center}
%\end{table}
%topic
%{\color{red} How the null plots were generated. }\\
%The 19 null plots in the lineup were obtained by  permuting the population indicator variable (the variable that indicates which population the data set belongs to) so that breaking any dependency between the indicator variable and the other variables. Two different statistics Wilk's $\lambda$ (\cite{JW02})  and the within sum of squares to between sum of squares ratio were calculated for each of the 20 plots in the lineup. \\
%{\color{red} The data recorded for the participants. How many participants were recruited.} \\
%Participants  for the experiment were recruited through Amazon Mechanical Turk. Each participant was shown a sequence of 9 lineups. They were asked to identify the plot which has the most separation between the colored groups, give a reason for their choice and determine the level of confidence for their decision.  Gender, age, educational qualification and location of each participant were also noted. In total, 1137 lineups were evaluated by 103 participants from different locations. \\

\section{Results of the Turk Experiment}

%topic
{\color{red} Description of the results. }\\

Participants  for the experiment were recruited through Amazon Mechanical Turk ( \cite{turk} ).  \cite{turk} is an amazing platform to get feedback on this kind of experiments. Each participant was shown a sequence of 10 lineups. Participants also had the freedom of answering more than 10 lineups. They were asked to identify the plot which has the most separation between the colored groups. Their response was recorded along with a reason for their choice of the plot and the level of confidence they have in their decision.  Gender, age, educational qualification and location of each participant were also noted. In total, 1137 lineups were evaluated by 103 participants from different locations.

\subsection{Cleaning of the data}

Amazon Mechanical turk participants are paid for their responses. Since the process is not monitored manually, some participants does not make an honest effort to find the actual plot but just picks plots randomly. To counter this problem, each participant responded to a comparatively easy lineup (a lineup with $p = 10$ dimensions with some real separation). The participants who failed to give a correct response to this lineup were removed from the study. If the response in this lineup is correct, we remove the response of this lineup but keep all the other responses. In this way we finally retain the response of 101 participants.  

\subsection{Effect of dimensions, presence of real separation and projections on responses} \label{effects}

We are interested in the proportion of participants identifying the correct plot in the presence of real separation versus pure noise, for 1d projections versus 2d projections and for the different dimensions.
Figure \ref{suc-rate-glm} shows the proportion of the participants who successfully picked the plot of the actual data for each replicate of each dimension. We can clearly see when there is some real separation , the rate of success in making the correct response overall is higher than the rate of success when the data is complete noise and there is no real separation. Also there is a difference in terms of the variability among the replicates in each dimension.
We notice that the rate of success is not very different for 1d and 2d projections both in case of data with real separation and also for noise data. %\subsection{1D vs 2D projections}
The rate of success in picking the correct plot is different for different values of the dimension $p$. The rate of success is higher for dimension $p = 20$ than when the dimension is $p = 40$. Strangely enough we notice that the rate of success for $p = 100$ is higher than the rate of success for $p=80$ for both 1d and 2d projections. 

\begin{figure}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{0.7}{\includegraphics{suc-rate-rep-glm.pdf}}
%        \scalebox{0.80}{\includegraphics{result-noise.pdf}}
      \caption{Plot showing the proportion of correct response for 1D projection and 2D projection and data with real separation and pure noise. The three points represents the three replicates for each dimension. A binomial generalized linear model is overlaid on the points. We can see that the proportion of correct response decreases as we increase the number of dimensions ($p$) from 20 to 100 for data with real separation. When the data is pure noise data, the proportion of correct response is very low which is understandable as we do not expect people to identify the plot when the data is pure noise.  }
       \label{suc-rate-glm}
\end{figure}

\subsection{Model fitting}

Suppose each of $K$ independent participants responded to multiple lineups and responses are considered to be a binary random variable $Y_{lk}$ where $$Y_{lk} \sim \hbox{Ber}(p_{lk})$$ where $Y_{lk} = 1$ if the $k$-th participant correctly identifies the actual plot in the $l$-th lineup, $1 \leq k \leq K$, $1 \leq l \leq L$ and 0 otherwise. We are interested in predicting the proportion of success in correctly identifying the actual plot within the range of the dimensions used i.e between $p = 20$ and $p = 100$. Treating dimensions as a continuous variable, a fixed effects logistic regression model is used for $P(Y_{lk} = 1) = p_{lk} = E(Y_{lk})$ with the different parameters of the lineup as covariate.
The model can be fit as:
\begin{equation}
g(p_{lk}) = \theta + \alpha_{lk} + \beta_{lk} + \gamma x_{lk} \label{fixed}
\end{equation}
where $\theta$ is the intercept, $\alpha_{lk}$ is the effect of noise on the $l$-th lineup for the $k$-th participant ,
$\beta_{lk}$ is the effect of projection on the $l$-th lineup for the $k$-th participant 
and $x_{lk}$ is the dimension of the $l$-th lineup for the $k$-th participant. $\gamma$ is the corresponding slope parameter and $g(.)$ denotes the logit link function $g(\pi)$ = log($\pi$) - log(1 - $\pi$); $0 \leq \pi \leq 1$.  We can invert the logit link function to estimate the lineup specific probability of successful evaluation as 
\begin{equation}
\hat{p}_{lk} = g^{-1}(\hat{\theta} + \hat{\alpha}_{lk} + \hat{\beta}_{lk} + \hat{\gamma} x_{lk}) \label{invert}
\end{equation}

We fit Model \ref{fixed} to our data and obtain the probabilities of successful evaluation for each lineup.  The blue line in Figure \ref{suc-rate-glm} shows the probability of successful evaluation for the different levels of the parameters. We notice that as we increase the number of dimensions $p$ in the lineups with real separation, the probability of success in picking the true plot decreases. This is understandable since as we increase the number of dimensions of noise keeping the number of real separation fixed to one or two, the distance between the groups in the actual plot get closer to the distance between the groups in a null plot making it difficult for the participants to pick the actual plot. Table \ref{params} shows the estimates of the parameters, the standard errors and the corresponding $p$-values.

\begin{table}[ht]
\begin{center}
\caption{Table showing the estimate, the standard error and the $p$-value of the parameters used in Model \ref{fixed}.}
\vspace{0.15cm}
\begin{tabular}{r|ccc}
\hline
  \hline
 Parameters & Estimate & Std. Error  & $p$-value \\ 
  \hline
Intercept  & 1.953 & 0.253  & 0.000 \\ 
  dimension & $-$0.026 & 0.003  & 0.000 \\ 
  noise1 & $-$3.750 & 0.298  & 0.000 \\ 
  projection2 & $-$0.079 & 0.176  & 0.653 \\ 
   \hline
\end{tabular}
\label{params}
\end{center}
\end{table}

In Table \ref{params} we see that the $p$-values corresponding to dimension and presence of real separation is very highly significant. But the $p$-value corresponding to the projection is not significant which we also suggested in the previous section.

\subsection{Does rotation in the 2d projections affect the responses?}

The 1d projections were plotted in a lineup making the adjustment that the group with the lower values of projection are always considered to be group 1 and the group with larger values of the projection are considered to be group 2, as can be seen in Figure \ref{fig:test_category_1d}. So the orientation of the groups does not affect the response of the subjects. But in case of 2d projections, the groups are rotated in a different way in each plot in the lineup (Figure \ref{fig:test_category}) which may influence the response of the participants. We were particularly interested to test the effect of the responses in the 2d projections. Since the 1d projections are adjusted, we can use the lineups of the 1d projections as a control and basically compare the responses for the 1d and 2d projections. Figure \ref{suc-rate-glm} shows that there is not much of a difference between the 1d and 2d projections both in presence and absence of real separation. This is also verified in Table \ref{params} where the effect of the projections is not significant at 5\% level.  

%\subsection{Response as the dimension increases}



\subsection{Subject-wise responses}

To look at the subject specific probability of successful evaluation we modified Model \ref{fixed} to bring in  a random effect for the individuals. Using the setup of Model \ref{fixed}, we have
\begin{equation}
g(p_{lk}) = \theta + \alpha_{lk} + \beta_{lk} + \gamma x_{lk} + u_k \label{mixed}
\end{equation}
where $u_k$ is a random effect specific to individual $k$, $k = 1 \leq k \leq K$ and $u_k \sim N(0, \sigma_u^2)$ independently. We can similarly invert the link function to obtain the probability of successful evaluations.

Using \textit{lme4} package in R, we fit a mixed effects logistic regression model specified in Model \ref{mixed}. The black lines in Figure \ref{subject-glm} shows the probability of successful evaluation of each individual with the blue line showing the fixed effect. We notice that some of the participants has a very high probability of success for both one and two dimensional projections. Also as the number of dimensions increases, the probability of successful evaluation decreases. Most of the individuals perform similarly when the data has no real separation. Also we can see that there is a higher variability in the individual responses for data with real separation but not much when the data is complete noise.

\begin{figure*}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{0.75}{\includegraphics{subjectwise-glm.pdf}}
%        \scalebox{0.80}{\includegraphics{result-noise.pdf}}
      \caption{Plot showing the subject wise probabilities for each participant with the blue line showing the average of all the people.}
       \label{subject-glm}
\end{figure*}

\subsection{Time taken to respond}

The amount of time taken to respond may have a positive relationship with the amount of difficulty in identifying the actual plot in the lineup. So we looked at the distribution of the time taken to respond for the different parameters. Figure \ref{time-taken} shows the distribution of the time taken to respond on the logarithm scale by the participants for the dimensions for the different projections. The colors suggest whether the data has some real separation or the data is complete noise. We can see that as the dimensions increases, the participants  takes more time to respond to the lineups when the data has some real separation. But when the data is complete noise, the increase of dimension does not have any effect on the time. This suggests that as the number of dimension of noise increases with fixed number of real separation, it becomes extremely hard to spot the actual plot among the null plots. On the other hand, the difficulty of spotting the actual plot for a data with complete noise does not vary with dimensions. It can also be seen that the time taken when the data is complete noise is overall higher than the time taken when the data has some real separation. There is not much of a difference between the time taken for 1-dimensional and 2-dimensional projections.



\begin{figure*}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{0.7}{\includegraphics{time-taken-log.pdf}}
%        \scalebox{0.80}{\includegraphics{result-noise.pdf}}
      \caption{Boxplots showing the distribution of the time taken to respond for dimension on log scale colored by the presence of real separation. We can observe that the time taken to respond is higher when the data is complete noise than when there is some real separation. Also we notice that as we increase the number of dimensions the difference between the time taken to respond for random noise data and real separation decreases. }
       \label{time-taken}
\end{figure*}

%\begin{figure*}[hbtp]
%%\begin{figurehere}
%   \centering
%       \scalebox{0.7}{\includegraphics{choice-reason-bar.pdf}}
%%        \scalebox{0.80}{\includegraphics{result-noise.pdf}}
%      \caption{Bar diagram showing the counts of people's choice of reasons colored by the data. Notice that people mostly pick the ``biggest gap" as their choice of reason when they are working with the data with real separation. Also some of the reasons like ``Groups are in corners" does not appear for 1D projections.   }
%       \label{real_noise}
%\end{figure*}

%\Large{\textit{What affects the decision of the people?}} \\[0.2cm]

\subsection{What affects the decision of the people?}

\normalsize
%topic
{\color{red} What do people see when they select a plot.} \\
%Participants were asked to identify the plots which has the most separated colored groups. In data sets with real separation, participants make mistakes in picking up the plot of the ``observed data". 

From Figure \ref{suc-rate-glm} we notice that some of the participants are able to identify the plot of actual data  although the data has no real separation i.e. the data is complete noise . This motivates us to look at the measures calculated for each plot in a lineup. Figure \ref{wbratio} plots the ratio of within to between sum of squares (WBratio) and the relative frequency of picks of each plot in the lineup.

In Figure \ref{wbratio}, we see that there is a negative correlation between the relative frequency of the picks and the WBratio. So as the WBratio increases, which indicates the decrease in the distance between the clusters,  the number of participants picking the plot decreases. Red indicates the actual plot. For data with real separation, when the actual plot has the smallest WBratio, the participants are successful in picking up the actual plot. But for data with no real separation participants are able to pick the plot with the lowest WBratio value although it is not the actual plot. Also we need to remember that Figure \ref{wbratio} has been drawn using a ``free scale" along the horizontal axis. So for higher dimensions and presence of real separation and also for complete noise data, the difference between the WBratio values for different plots in a lineup are really small compared to the lineup for a lower dimension and data with real separation.  


\begin{figure*}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{0.7}{\includegraphics{wbratio.pdf}}
       \caption{Relative frequency of picks compared to other plots in the lineup plotted against WBratio on a free scale. Red indicates the actual plot. The columns shows the combination of dimension and projection and the row indicates the combination of replication and the presence of noise.}
       \label{wbratio}
\end{figure*}

%We made 5 lineups of each of the above plots for each dimension and showed to different individuals. They were asked to pick up the ``true" one among the 20 plots. By the ``true" one, we meant the one with the most separated colors. 20\% of the individuals could identify the true plot when the data is 100 dimensions of pure noise for one dimensional projections. But when there is one dimension of separation in 100 dimensions of pure noise, 90\% of the individuals could successfully identify the true plot. For the two dimensional projections, only 30\% of the individuals could identify the true plot when the data has 100 dimensions of pure noise but 60\% of the individuals can pick the correct plot when there is two dimensions of real separation. It is evident that people are mostly correct in selecting the true plot when one dimension among 100 dimensions have real separation. People are also correct to pick the plots when two dimensions among 100 have real separation.



%\subsection{Comparisons with real separation}
%
%We also created plots with 99 dimensions of pure noise and one dimension of real separation, in all making it 100 dimensions. In the same way, each dimension has 30 observations and they were divided into 2 classes. The separation was designed in a way such that if we plot the one dimension with real separation, the points lie approximately 6 units apart. For plotting the two dimensional projections, we considered 98 dimensions of pure noise and 2 dimensions of real separation. Again each dimension has 30 observations with 3 classes. The two dimensions with real separation was constructed such that the points lies close to the vertex of an equilateral triangle of side 6 units.
%
%We made 5 lineups of such plots for each dimension and showed them to the same individuals. They were again asked to pick the plot with the most separated colors. The proportion of correct choices were 0.9 and 0.6 respectively for the one and two dimensional projections. Figure \ref{fig:result} shows the proportion of correct response by pure noise only or presence of known real separation. It is evident that people are mostly correct in selecting the true plot when one dimension among 100 dimensions have real separation. People are also correct to pick the plots when two dimensions among 100 have real separation.

%\end{multicols}

%\begin{figure}[hbtp]
%%\begin{figurehere}
%   %\centering
%       \centerline{\includegraphics[width=0.25\textwidth]{result_1d_noise.pdf}\includegraphics[width=0.25\textwidth]{result_1d_sep.pdf}}
%       \centerline{\includegraphics[width=0.25\textwidth]{result_2d_noise.pdf}\includegraphics[width=0.25\textwidth]{result_2d_sep.pdf}}
%       \caption{Barplots showing the number of correct responses for one and two dimensional projections for data with pure noise and data with real known separation. People are more often correct in selecting the true plot when there is some real separation.  }
%       \label{fig:result}
%\end{figure}
%%\end{figurehere}



%\begin{multicols}{2}

\subsection{How does the null plots affect the response ?}

In classical inference the test statistic can be compared with an infinite number of possible values from the sampling distribution under the null hypothesis. But in visual inference the participants can compare the test statistic (the observed plot) with only a finite number of null plots which makes the null plots an important factor in decision making. We have mentioned in Section \ref{effects} that the probability of correct evaluation decreases for dimension $p = 80$ and increases for dimension $p = 100$ which is opposite to what we should expect. To investigate this we look at the difference between the minimum WBratio of the null plots and the WBratio for the observed plot for each lineup. Since the values of WBratio vary for each lineup depending on the presence or absence of real separation, we scale it by dividing the difference by the WBratio of the observed plot of the lineup. Figure \ref{null} shows the relationship between the difference and the probability of successful evaluation.  

\begin{figure*}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{0.7}{\includegraphics{suc-diff-wbratio.pdf}}
       \caption{Proportion of successful evaluation is plotted against the relative difference between the minimum WBratio of the null plots and the WBratio of the observed plot for each lineup. The black line gives the 0 difference line. The points left to the zero line indicates a difficult lineup in the sense that a null plot had a lower WBratio value than the observed plot. }
       \label{null}
\end{figure*}

Figure \ref{null} shows that for data with real separation dimension has an effect on the difficulty level but for data with noise there is no effect of dimension. For some lineups with noise data the observed plot has a lower WBratio value than the null plots. It can be observed from Figure \ref{null} that the participants could pick the observed plot if it has the lowest WBratio value. The unexpected performance of the participants when the dimension  is $p = 80$ can be explained by the fact that the one or more of the null plots have lower WBratio value than the observed plot.


\section{Distance Increasing with $p$ for Fixed $n$} \label{sec:distance}

\subsection{Procedure}
%topic
{\color{red} Description of the procedure to look at the distance between the groups as we increase the number of dimension for a data with no real separation.} \\
Consider two dimensions of pure noise, each dimension having 30 observations divided into 2 classes with 15 observations in each class. If we perform the projection pursuit on this and plot the one dimensional projection pursuits with different color for the classes, we notice an overlap of all the colors in the plot. It signifies that there is no real class and hence the colors overlap. But as we increase the number of dimensions for fixed n, we notice the colors starts separating out and the clusters are formed. Figure \ref{dist_1d} shows the one dimensional projections for $p=2$, $p=20$, $p=50$ and $p=100$. \\

%\end{multicols}
%
%%\newpage
%%
\begin{figure*}[hbtp]
%\begin{figurehere}
   \centering
	\scalebox{0.25}{\includegraphics{plot_1d_2.pdf}}
	\scalebox{0.25}{\includegraphics{plot_1d_20.pdf}}
	\scalebox{0.25}{\includegraphics{plot_1d_50.pdf}}
	\scalebox{0.25}{\includegraphics{plot_1d_100.pdf}}
 \scalebox{0.25}{\includegraphics{plot_2d_3.pdf}}
	\scalebox{0.25}{\includegraphics{plot_2d_20.pdf}}
	\scalebox{0.25}{\includegraphics{plot_2d_50.pdf}}
	\scalebox{0.25}{\includegraphics{plot_2d_100.pdf}}
       \caption{Plots showing one and two dimensional projections for 4 different number of dimensions $p=3$, $p=20$, $p=50$ and $p=100$. Note that the classes look more and more separated as we increase the number of dimensions when actually there is no real separation.  }
       \label{dist_1d}
\end{figure*}

%\begin{multicols}{2}
{\color{red} Description for two dimensional projections.} \\
Again consider 3 dimensions of pure noise, each dimension having 30 observations divided into 3 classes with 10 observations in each class. We again perform the projection pursuit on this and plot the two dimensional projections. Similarly, in this case the 3 colors starts separating out as we increase the number of dimensions.  Figure \ref{dist_1d} also shows the two dimensional projections for $p=3$, $p=20$, $p=50$ and $p=100$.

%\end{multicols}



%\begin{figure*}[hbtp]
%%\begin{figurehere}
%   \centering
%	\scalebox{0.25}{\includegraphics{plot-true3.pdf}}
%	\scalebox{0.25}{\includegraphics{plot-true20.pdf}}
%	\scalebox{0.25}{\includegraphics{plot-true50.pdf}}
%	\scalebox{0.25}{\includegraphics{plot-true100.pdf}}
%       \caption{Plots showing two dimensional projections for 4 different number of dimensions $p=3$, $p=20$, $p=50$ and $p=100$. Note that, like the one dimensional projections, the classes look more and more separated as we increase the number of dimensions when actually there is no real separation.  }
%       \label{dist_2d}
%\end{figure*}

%\begin{multicols}{2}

\section{Wasps data, revisited. }

%\subsection{Statement of the hypothesis}

We obtained the wasp data used to generate Figure \ref{oligo} from the authors.  As we have seen earlier in Figure \ref{oligo} that at least two of the groups are separated from the others.  The important question is ``Is this separation real or fake ? ". We can start with a null hypothesis $H_0$: The separation seen in the groups in Figure \ref{oligo} is fake versus the alternative $H_a$: The separation seen in the groups is real. 

%\subsection{Description of the procedure}

To test the above hypothesis, we use the wasp data obtained from the authors (of \cite{toth:2010}) to get LD1 and LD2 from a LDA. As the authors did, we plot LD1 versus LD2 in a scatterplot colored by the four groups. This works as my actual plot. We generate the null plots by randomly permuting the group variables so that any dependence between the groups and the other variables are broken. Then we perform a LDA on the permuted data and plot LD1 versus LD2 to obtain the null plots. The actual plot is placed randomly among the null plots to get a lineup. Figure \ref{toth_lineup} shows such a lineup.


\begin{figure*}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{1}{\includegraphics{toth_lineup_lda.pdf}}
       \caption{Lineup Plot showing LD1 versus LD2 from an LDA on a randomly selected subset of 40 significantly different oligos. F, Foundress; G, gyne; Q, queen and W, worker. The actual data plot is placed randomly among the 19 null plots. Can you identify the actual data?  }
       \label{toth_lineup}
\end{figure*} 

To compare our previous lineup, we generate a permuted data from the wasp dataset by permuting the group variable. Using the permuted data, we perform LDA and obtain LD1 and LD2 and plot them to obtain the actual plot. To generate the null plots, we again permute the group variable of this data and perform LDA to obtain LD1 and LD2 and plot them in a scatterplot. The actual plot is randomly placed among the other 19 null plots in the lineup. 

So we have two different lineups : one where the actual data is the wasp data and the other where the actual data is ``permuted data" obtained by permuting the wasp data. We made three replicates of each. Participants from the Amazon Turk was shown one of the above lineups and were asked to identify the plot which has the most separation among the colored groups. They were also asked to record a reason for their choice and also a confidence level on the scale of 1 to 5.

%\subsection{Results}

The probability of successful evaluation for the wasp data and permuted data is shown in Figure \ref{wasp-result}. As we can see, none of the participants were able to identify the actual plot in the wasp data. But around 10\% of the participants were able to identify the actual plot when we used the permuted data to generate the lineup.

\begin{figure*}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{1}{\includegraphics{wasp-result.pdf}}
       \caption{Barplot showing the probability of successful evaluation for the Wald data and the permuted data. The error bar gives the 95\% adjusted Wald intervals. }
       \label{wasp-result}
\end{figure*} 

In Figure \ref{toth_lineup} we can see that the clusters are separated even after breaking any dependency between the groups and the variables. Since PDA is a better index than LDA in a large $p$ situation, we used PDA to generate another lineup which is presented in Figure \ref{toth_pda}. In Figure \ref{toth_pda}, the clusters are not even separated and we cannot identify the true plot. We also generated 40 dimensions of random noise with 50 observations in each dimension. A group variable is also added as the one present in the wasp data. We performed a LDA on this random noise data and we can still see clusters.  This shows that the separation seen in Figure \ref{oligo} may not be real but appear only due to the fact that the number of variables are large.

\begin{figure*}[hbtp]
%\begin{figurehere}
   \centering
       \scalebox{1}{\includegraphics{toth_lineup_pda.pdf}}
       \caption{Lineup Plot showing PD1 versus PD2 from an PDA on a randomly selected subset of 40 significantly different oligos. F, Foundress; G, gyne; Q, queen and W, worker. The actual data plot is placed randomly among 19 null plots. Can you identify the actual data?  }
       \label{toth_pda}
\end{figure*}   



%
%\subsection{Fun Activity}
%
%{\color{red} The description of the procedure to calculate a probable bound for the number of dimension at which the groups start separating. } \\
%The previous section clearly suggests that the clusters becomes more and more visible as we increase the number of dimensions. So a natural question is ``What is the probable dimension of data when we can clearly see the two separated clusters for the one dimensional projections and the three separated clusters for the two dimensional projections." So out of our curiosity,  we consider 2 dimensions of random noise and each dimension has 30 observations with 2 classes. Hence there is 15 observations in each class. We plot the one dimensional projections and see if we can see the classes separated. We say that the two class are separated if we can draw a straight line between the two colors without touching any of them. We repeat the same procedure by increasing the number of dimensions by 1 every time. In this manner we plot the two dimensional projections for the number of dimensions from 2 to 62. Figure \ref{conf_int} shows the plots of the one dimensional projections for the number of dimensions from 23 to 42 and Figure \ref{conf_int1} shows the plots of the two dimensional projections for $p$ ranging from 23 to 42.
%
%%\end{multicols}
%
%\begin{figure*}[hbtp]
%%\begin{figurehere}
%   \centering
%       \scalebox{0.95}{\includegraphics{plot_1d_22_41_new.pdf}}
%       \caption{Plots showing the one dimensional projections for a fixed sample size $n=30$ and the number of dimensions ranging from 23 to 42. The number written above the plot is the number of dimension. Can you see where the colors starts separating?  }
%       \label{conf_int}
%\end{figure*}
%
%\begin{figure*}[hbtp]
%%\begin{figurehere}
%   \centering
%       \scalebox{0.95}{\includegraphics{plot_2d_23_42_new.pdf}}
%       \caption{Plots showing the two dimensional projections for a fixed sample size $n=30$ and the number of dimensions ranging from 23 to 42. The number written above the plot is the number of dimension. Can you see where the colors starts separating?  }
%       \label{conf_int1}
%\end{figure*}
%
%%\begin{multicols}{2}
% 
%Now we see the plot where the one dimensional projections shows a separation between the colors. We draw 100 such plots and note the number of dimension at which the classes separate out. We take the 5th and the 95th percentile of these numbers to obtain a probable range of the dimension at which the clusters are formed.
%
%Similarly we take 3 dimensions of pure noise with 30 observations in each dimension. Each dimension is divided into 3 classes, with 10 observations in each class. We plot the two dimensional projections and see whether the classes are separated. We again define the classes to be separate if we can draw three lines making an angle of $120^{\circ}$ among themselves without touching any color. So the classes will be separated if none of the colors overlap. We again repeat the same procedure by increasing the number of dimensions by 1 every time. We plot the two dimensional projections from 3 to 62 for sample size 30. We again make 100 such plots and note the number of dimension at which the classes start separating. 
%
%{\color{red} Explanation of the findings. } \\
%Hence a probable range that we obtain from the 100 plots for the one dimensional projection is between 24 and 34 and the a probable range for the two dimensional projection is between 26 and 36. 

\section{Conclusions}
{\color{red} Concluding remarks summarizing the different findings.} \\
The purpose of this paper has been to utilize  visual inference methods to examine the reliability of the projection pursuit results. We found that there were no problems with the projection pursuit: that viewers had more difficulty picking the ``true" data when it was pure noise. We also use the visual inference method to examine when clusters start to form for a large number of dimensions of pure noise for a fixed sample size. The classes are more and more separated as we increase the number of dimensions. Being motivated by the separation of the classes, we looked for a probable range of the number of dimensions at which the classes start separating for both one dimensional and two dimensional projections, in order to compare with the theoretical predictions. These were between 24-34 for one dimensional projections, and 26-36 for two dimensional projections.

\paragraph{Acknowledgement:}
%
This work was funded by National Science Foundation grant DMS 1007697.

\bibliographystyle{plainnat}
%%\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
