\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{buja:2009}
\citation{hall:2005}
\citation{marron:2007}
\citation{marron:2007}
\newlabel{ch:largepsmalln}{{}{1}{\relax }{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{marron:2011}
\citation{yata:2010}
\citation{donoho:2009}
\citation{donoho:2008}
\citation{toth:2010}
\citation{dudoit:2002}
\citation{buja:2009}
\citation{majumder:2011}
\citation{toth:2010}
\citation{toth:2010}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces LD1 versus LD2 from an LDA on a randomly selected subset of 40 significantly different oligos : F, Foundress; G, gyne; Q, queen and W, worker. It can be noticed that the groups F and G are separated. This plot is generated to match Figure 2 in \cite  {toth:2010}. }}{2}{figure.1}}
\newlabel{oligo}{{1}{2}{LD1 versus LD2 from an LDA on a randomly selected subset of 40 significantly different oligos : F, Foundress; G, gyne; Q, queen and W, worker. It can be noticed that the groups F and G are separated. This plot is generated to match Figure 2 in \cite {toth:2010}}{figure.1}{}}
\citation{buja:2009}
\citation{gelman:2004}
\citation{buja:2009}
\citation{buja:2009}
\citation{majumder:2011}
\citation{turk}
\citation{majumder:2011}
\citation{friedman:1974}
\citation{huber:1985}
\@writefile{toc}{\contentsline {section}{\numberline {2}Visual inference methods}{3}{section.2}}
\newlabel{sec:inference}{{2}{3}{Visual inference methods\relax }{section.2}{}}
\citation{lee:2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A typical lineup ($m = 20$) for testing $H_o: \mu _1 = \mu _2$. When the alternative hypothesis is true the observed data plot should have the largest vertical difference between the centers. Can you identify the observed data plot? The solution to the lineup is provided in the Appendix.}}{4}{figure.2}}
\newlabel{lineup}{{2}{4}{A typical lineup ($m = 20$) for testing $H_o: \mu _1 = \mu _2$. When the alternative hypothesis is true the observed data plot should have the largest vertical difference between the centers. Can you identify the observed data plot? The solution to the lineup is provided in the Appendix}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Explanation of the methods}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dimension reduction}{4}{subsection.3.1}}
\citation{r}
\citation{WC08}
\citation{lee:2009}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of visual inference with traditional hypothesis testing.}}{5}{table.1}}
\newlabel{tbl:compare}{{1}{5}{Comparison of visual inference with traditional hypothesis testing}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experimental Setup}{5}{subsection.3.2}}
\newlabel{sec:experiment}{{3.2}{5}{Experimental Setup\relax }{subsection.3.2}{}}
\citation{hennig:2010}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Values of parameters considered for the experiment.}}{6}{table.2}}
\newlabel{freq}{{2}{6}{Values of parameters considered for the experiment}{table.2}{}}
\citation{JW02}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Lineup ($m=20$) with 19 dimensions of purely noise and 1 dimension of real separation. So the lineup is generated from a data with 1 dimension of real separation. The subjects were asked to identify the plot with the most separated colors. Can you identify the observed data plot? The solution to the lineup is provided in the Appendix. }}{7}{figure.3}}
\newlabel{fig:test_category_1d}{{3}{7}{Lineup ($m=20$) with 19 dimensions of purely noise and 1 dimension of real separation. So the lineup is generated from a data with 1 dimension of real separation. The subjects were asked to identify the plot with the most separated colors. Can you identify the observed data plot? The solution to the lineup is provided in the Appendix}{figure.3}{}}
\citation{turk}
\citation{turk}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Lineup ($m=20$) with 100 dimensions of purely noise. In this case the lineup is generated from a data with 100 dimensions of purely noise. The subjects were asked to identify the plot with the most separation between the colored groups. Can you identify the observed data plot? The solution is provided in the Appendix. }}{8}{figure.4}}
\newlabel{fig:test_category}{{4}{8}{Lineup ($m=20$) with 100 dimensions of purely noise. In this case the lineup is generated from a data with 100 dimensions of purely noise. The subjects were asked to identify the plot with the most separation between the colored groups. Can you identify the observed data plot? The solution is provided in the Appendix}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Theory}{9}{subsection.3.3}}
\newlabel{sec:theory}{{3.3}{9}{Theory\relax }{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Plot showing the distribution of the sum of absolute difference of means for data with purely noise ($Y$) and real separation ($Z$) for different values of the dimension. The distribution of $Y$ and $Z$ are colored in red and blue respectively with the dark blue line showing the 5th percentile of $Z$. The dark blue area shows the area of $Y$ which is greater than the 5th percentile of $Z$. It can be noticed that the area of the dark blue region increases as the number of dimensions increases. This indicates that the probability of obtaining the sum of absolute difference for data with purely noise similar to the difference for data with real separation increases with $p$. }}{11}{figure.5}}
\newlabel{fig:dimen}{{5}{11}{Plot showing the distribution of the sum of absolute difference of means for data with purely noise ($Y$) and real separation ($Z$) for different values of the dimension. The distribution of $Y$ and $Z$ are colored in red and blue respectively with the dark blue line showing the 5th percentile of $Z$. The dark blue area shows the area of $Y$ which is greater than the 5th percentile of $Z$. It can be noticed that the area of the dark blue region increases as the number of dimensions increases. This indicates that the probability of obtaining the sum of absolute difference for data with purely noise similar to the difference for data with real separation increases with $p$}{figure.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Numerical summaries of dimension $p$ for each value of $\delta $. Notice that with the increase in the dark blue region $\delta $, the median number of dimensions required to obtain the region also increases. This also means for a large value of $p$ the difference between the groups for data with purely noise is close to the difference between the groups for data with real separation. }}{11}{table.3}}
\newlabel{tab:dimen}{{3}{11}{Numerical summaries of dimension $p$ for each value of $\delta $. Notice that with the increase in the dark blue region $\delta $, the median number of dimensions required to obtain the region also increases. This also means for a large value of $p$ the difference between the groups for data with purely noise is close to the difference between the groups for data with real separation}{table.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{12}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Data cleaning}{12}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Effect of experimental factors}{12}{subsection.4.2}}
\newlabel{effects}{{4.2}{12}{Effect of experimental factors\relax }{subsection.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Table showing the estimate, the standard error and the $p$-value of the parameters used in logistic regression model. Notice that the covariates dimension and noise are highly significant while projection is not significant at 5\% level of significance. Also the interaction term between dimension and noise is also significant.}}{12}{table.4}}
\newlabel{params}{{4}{12}{Table showing the estimate, the standard error and the $p$-value of the parameters used in logistic regression model. Notice that the covariates dimension and noise are highly significant while projection is not significant at 5\% level of significance. Also the interaction term between dimension and noise is also significant}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Proportion of correct responses for 1D projection and 2D projection and data with real separation and purely noise are shown. The three points represents the three replicates for each treatment level. A fixed effects logistic regression model is overlaid on the points. It can be seen that the proportion of correct response decreases as the number of dimensions ($p$) increases from 20 to 100 for data with real separation. When the data is purely noise data, the proportion of correct response is very low which is understandable as the subjects are not expected to identify the plot. }}{13}{figure.6}}
\newlabel{suc-rate-glm}{{6}{13}{Proportion of correct responses for 1D projection and 2D projection and data with real separation and purely noise are shown. The three points represents the three replicates for each treatment level. A fixed effects logistic regression model is overlaid on the points. It can be seen that the proportion of correct response decreases as the number of dimensions ($p$) increases from 20 to 100 for data with real separation. When the data is purely noise data, the proportion of correct response is very low which is understandable as the subjects are not expected to identify the plot}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Does rotation in the 2D projections affect the responses?}{13}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Time taken to respond}{13}{subsection.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Plot showing the time taken to respond on log scale by each dimension colored by the type of the data. A loess curve is fit through the points. It can be observed that the time taken to respond is higher when the data is purely noise than when there is some real separation. Also it can be noticed that as the number of dimensions increases, the difference between the time taken to respond for purely noise data and real separation decreases. }}{14}{figure.7}}
\newlabel{time-taken}{{7}{14}{Plot showing the time taken to respond on log scale by each dimension colored by the type of the data. A loess curve is fit through the points. It can be observed that the time taken to respond is higher when the data is purely noise than when there is some real separation. Also it can be noticed that as the number of dimensions increases, the difference between the time taken to respond for purely noise data and real separation decreases}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}What affects decisions?}{14}{subsection.4.5}}
\citation{roychowdhury:2012}
\citation{ripley:1996}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}How do the null plots affect choices?}{15}{subsection.4.6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Distance Increasing with $p$ for Fixed $n$}{15}{section.5}}
\newlabel{sec:distance}{{5}{15}{Distance Increasing with $p$ for Fixed $n$\relax }{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparing the choices that subjects make for each lineup. Relative frequency of plots chosen against a measure of the distance between means, WBratio, the smaller the value the more separated are the groups. Each cell here shows the data for one of the lineups used in the experiment, 60 in total, and each ``pin'' represents a plot in the lineup, 20 for each lineup. Red indicates the observed data plot. Subjects were asked to pick the plot in the lineup where the groups were the most separated, so we would expect that more subjects would pick the plots with the smallest WBratio. In general, this happens, the tallest pins are in the left of each cell. The top three rows show the results for the data with real separation, so the observed data plot (red) is typically the pin on the very left of the cell, less so for the higher dimensions which are the cells at right. Also the figure on the top is for 1D projections and on the bottom is for 2D projections. We do not see much difference between the two figures. }}{16}{figure.8}}
\newlabel{wbratio}{{8}{16}{Comparing the choices that subjects make for each lineup. Relative frequency of plots chosen against a measure of the distance between means, WBratio, the smaller the value the more separated are the groups. Each cell here shows the data for one of the lineups used in the experiment, 60 in total, and each ``pin'' represents a plot in the lineup, 20 for each lineup. Red indicates the observed data plot. Subjects were asked to pick the plot in the lineup where the groups were the most separated, so we would expect that more subjects would pick the plots with the smallest WBratio. In general, this happens, the tallest pins are in the left of each cell. The top three rows show the results for the data with real separation, so the observed data plot (red) is typically the pin on the very left of the cell, less so for the higher dimensions which are the cells at right. Also the figure on the top is for 1D projections and on the bottom is for 2D projections. We do not see much difference between the two figures}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Proportion of successful evaluation is plotted against the ratio of the minimum WBratio of the null plots and the WBratio of the observed data plot for each lineup. The vertical line represents the ratio when the WBratio of the observed data plot is equal to the minimum WBratio of the null plots. The points left to the zero line indicates a difficult lineup in the sense that at least one of the null plots had a lower WBratio value than the observed data plot. The blue line shows a logistic regression model which indicates that there is a positive effect of the ratio on the proportion correct.}}{17}{figure.9}}
\newlabel{null}{{9}{17}{Proportion of successful evaluation is plotted against the ratio of the minimum WBratio of the null plots and the WBratio of the observed data plot for each lineup. The vertical line represents the ratio when the WBratio of the observed data plot is equal to the minimum WBratio of the null plots. The points left to the zero line indicates a difficult lineup in the sense that at least one of the null plots had a lower WBratio value than the observed data plot. The blue line shows a logistic regression model which indicates that there is a positive effect of the ratio on the proportion correct}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Plots showing one and two dimensional projections for 4 different number of dimensions $p=3$, $p=20$, $p=50$ and $p=100$. Note that the groups look more and more separated as the number of dimensions increases when actually there is no real separation and the data is purely noise. }}{17}{figure.10}}
\newlabel{dist_1d}{{10}{17}{Plots showing one and two dimensional projections for 4 different number of dimensions $p=3$, $p=20$, $p=50$ and $p=100$. Note that the groups look more and more separated as the number of dimensions increases when actually there is no real separation and the data is purely noise}{figure.10}{}}
\citation{toth:2010}
\citation{majumder:2011}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Probability that $n =30$ and $n = 50$ randomly chosen observations with $p$ dimensions randomly divided into two groups are linearly separable. For $n = 30$, when $p = 14$, the probability of obtaining linearly separable groups is 0.5 and for $n = 50$, the probability is 0.5 when $p = 24$ which are represented by the blue vertical lines. }}{18}{figure.11}}
\newlabel{combin}{{11}{18}{Probability that $n =30$ and $n = 50$ randomly chosen observations with $p$ dimensions randomly divided into two groups are linearly separable. For $n = 30$, when $p = 14$, the probability of obtaining linearly separable groups is 0.5 and for $n = 50$, the probability is 0.5 when $p = 24$ which are represented by the blue vertical lines}{figure.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Wasps data, revisited. }{18}{section.6}}
\citation{toth:2010}
\citation{lee:2009}
\citation{toth:2010}
\citation{toth:2007}
\citation{roychowdhury:2012}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Lineup showing LD1 versus LD2 from an LDA on a randomly selected subset of 40 significantly different oligos. F, Foundress; G, gyne; Q, queen and W, worker. The observed data plot is placed randomly among the 19 null plots. Which plot shows the most separation between the 4 groups? The solution is provided in the Appendix.}}{19}{figure.12}}
\newlabel{toth_lineup}{{12}{19}{Lineup showing LD1 versus LD2 from an LDA on a randomly selected subset of 40 significantly different oligos. F, Foundress; G, gyne; Q, queen and W, worker. The observed data plot is placed randomly among the 19 null plots. Which plot shows the most separation between the 4 groups? The solution is provided in the Appendix}{figure.12}{}}
\bibstyle{spbasic}
\bibdata{references}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Results of the Turk study on the wasps data. Proportion of successful evaluations of each lineup is shown, with the number of subjects, and $p$-value associated with the result. Notice that the most success came from one of the purely noise lineups, which occurred because the plot with the most difference between groups happened to be the one that was randomly generated as the ``real'' data. Averaging the $p$-values for each set of lineups, for the wasps was 1.0, and for the pure noise is 0.67 suggests that the apparent separation in the wasp data is consistent with pure noise induced by the high dimensions.}}{20}{table.5}}
\newlabel{wasp}{{5}{20}{Results of the Turk study on the wasps data. Proportion of successful evaluations of each lineup is shown, with the number of subjects, and $p$-value associated with the result. Notice that the most success came from one of the purely noise lineups, which occurred because the plot with the most difference between groups happened to be the one that was randomly generated as the ``real'' data. Averaging the $p$-values for each set of lineups, for the wasps was 1.0, and for the pure noise is 0.67 suggests that the apparent separation in the wasp data is consistent with pure noise induced by the high dimensions}{table.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{20}{section.7}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Appendix}{20}{section.8}}
\bibcite{turk}{{1}{2010}{{Amazon}}{{}}}
\bibcite{buja:2009}{{2}{2009}{{Buja et~al.}}{{Buja, Cook, Hofmann, Lawrence, Lee, Swayne, and Wickham}}}
\bibcite{donoho:2008}{{3}{2008}{{Donoho and Jin}}{{}}}
\bibcite{donoho:2009}{{4}{2009}{{Donoho and Jin}}{{}}}
\bibcite{dudoit:2002}{{5}{2002}{{Dudoit et~al.}}{{Dudoit, Fridlyand, and Speed}}}
\bibcite{friedman:1974}{{6}{1974}{{Friedman and Tukey}}{{}}}
\bibcite{gelman:2004}{{7}{2004}{{Gelman}}{{}}}
\bibcite{hall:2005}{{8}{2005}{{Hall et~al.}}{{Hall, Marron, and Neeman}}}
\bibcite{hennig:2010}{{9}{2010}{{Hennig}}{{}}}
\bibcite{huber:1985}{{10}{1985}{{Huber}}{{}}}
\bibcite{JW02}{{11}{2002}{{Johnson and Wichern}}{{}}}
\bibcite{marron:2011}{{12}{2012}{{Jung et~al.}}{{Jung, Sen, and Marron}}}
\bibcite{lee:2009}{{13}{2009}{{Lee and Cook}}{{}}}
\bibcite{majumder:2011}{{14}{}{{Majumder et~al.}}{{Majumder, Hofmann, and Cook}}}
\bibcite{marron:2007}{{15}{2007}{{Marron et~al.}}{{Marron, Todd, and Ahn}}}
\bibcite{r}{{16}{2009}{{R Development Core Team}}{{}}}
\bibcite{ripley:1996}{{17}{1996}{{Ripley}}{{}}}
\bibcite{roychowdhury:2012}{{18}{2012}{{Roy~Chowdhury et~al.}}{{Roy~Chowdhury, Cook, Hofmann, and Majumder}}}
\bibcite{toth:2007}{{19}{2007}{{Toth et~al.}}{{Toth, Varala, Newman, Miguez, Hutchison, Willoughby, Simons, Egholm, Hunt, Hudson, and Robinson}}}
\bibcite{toth:2010}{{20}{2010}{{Toth et~al.}}{{Toth, Varala, Henshaw, Rodriguez-Zas, Hudson, and Robinson}}}
\bibcite{WC08}{{21}{2010}{{Wickham and Cook}}{{}}}
\bibcite{yata:2010}{{22}{2011}{{Yata and Aoshima}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Acknowledgement:}{21}{section.9}}
